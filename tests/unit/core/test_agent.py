"""
Tests for Agent functionality.

These tests define the expected correct behavior for the Agent class,
which is the core autonomous agent that manages conversation flow,
tool execution coordination, and state management.
"""

import pytest
import asyncio
from unittest.mock import Mock, AsyncMock, patch, MagicMock
from datetime import datetime

from agentx.core.agent import Agent, AgentConfig, AgentState
from agentx.core.brain import Brain, BrainConfig, BrainResponse
from agentx.core.config import BrainConfig
from agentx.utils.logger import get_logger


class TestAgentInitialization:
    """Test Agent initialization and configuration."""
    
    def test_agent_requires_config(self):
        """Agent should require an AgentConfig."""
        config = AgentConfig(
            name="test_agent",
            description="Test agent",
            brain_config=BrainConfig(provider="openai", model="gpt-4")
        )
        agent = Agent(config)
        
        assert agent.config is config
        assert agent.name == "test_agent"
        assert agent.description == "Test agent"
        assert isinstance(agent.brain, Brain)
        assert isinstance(agent.state, AgentState)
        assert agent.state.agent_name == "test_agent"
    
    def test_agent_invalid_initialization(self):
        """Agent should reject invalid initialization."""
        with pytest.raises((TypeError, AttributeError)):
            Agent(None)
        
        with pytest.raises((TypeError, AttributeError)):
            Agent("not_a_config")
    
    def test_agent_with_default_brain_config(self):
        """Agent should use default brain config if none provided."""
        config = AgentConfig(name="test_agent", description="Test")
        agent = Agent(config)
        
        assert isinstance(agent.brain, Brain)
        assert isinstance(agent.brain.config, BrainConfig)
        # Should use default DeepSeek config
        assert agent.brain.config.provider == "deepseek"
        assert agent.brain.config.model == "deepseek-chat"
    
    def test_agent_with_custom_brain_config(self):
        """Agent should use custom brain config when provided."""
        brain_config = BrainConfig(
            provider="openai",
            model="gpt-4",
            temperature=0.5
        )
        config = AgentConfig(
            name="test_agent",
            description="Test",
            brain_config=brain_config
        )
        agent = Agent(config)
        
        assert agent.brain.config is brain_config
        assert agent.brain.config.provider == "openai"
        assert agent.brain.config.model == "gpt-4"
        assert agent.brain.config.temperature == 0.5
    
    def test_agent_with_tools(self):
        """Agent should handle tool configuration."""
        config = AgentConfig(
            name="test_agent",
            description="Test with tools",
            tools=["search_web", "file_write"]
        )
        agent = Agent(config)
        
        assert agent.tools == ["search_web", "file_write"]
        assert len(agent.tools) == 2
    
    def test_agent_with_tool_manager(self):
        """Agent should accept tool manager injection."""
        config = AgentConfig(name="test_agent", description="Test")
        mock_tool_manager = Mock()
        mock_tool_manager.get_builtin_tools.return_value = ["file_read", "file_write"]
        mock_tool_manager.get_tool_schemas.return_value = []
        
        agent = Agent(config, tool_manager=mock_tool_manager)
        
        assert agent.tool_manager is mock_tool_manager
    
    def test_agent_state_initialization(self):
        """Agent should initialize with correct state."""
        config = AgentConfig(name="test_agent", description="Test")
        agent = Agent(config)
        
        assert agent.state.agent_name == "test_agent"
        assert agent.state.is_active is False
        assert agent.state.current_step_id is None
        assert agent.state.tool_calls_made == 0
        assert agent.state.tokens_used == 0
        assert agent.state.errors_encountered == 0
        assert isinstance(agent.state.metadata, dict)
    
    def test_agent_memory_configuration(self):
        """Agent should handle memory configuration."""
        config = AgentConfig(
            name="test_agent",
            description="Test",
            memory_enabled=False,
            max_iterations=5
        )
        agent = Agent(config)
        
        assert agent.memory_enabled is False
        assert agent.max_iterations == 5
    
    def test_agent_warns_on_tool_function_calling_mismatch(self):
        """Agent should warn when tools are configured but brain doesn't support function calling."""
        brain_config = BrainConfig(
            provider="openai",
            model="gpt-3.5-turbo",
            supports_function_calls=False
        )
        config = AgentConfig(
            name="test_agent",
            description="Test",
            brain_config=brain_config,
            tools=["search_web"]
        )
        
        with patch('agentx.core.agent.logger.warning') as mock_warning:
            agent = Agent(config)
            
            # Should warn about mismatch
            mock_warning.assert_called_once()
            assert "function calling" in mock_warning.call_args[0][0]


class TestAgentState:
    """Test Agent state management."""
    
    def test_agent_state_model(self):
        """AgentState should be a proper Pydantic model."""
        state = AgentState(
            agent_name="test_agent",
            current_step_id="step_123",
            is_active=True,
            last_response="Test response",
            last_response_timestamp=datetime.now(),
            tool_calls_made=3,
            tokens_used=150,
            errors_encountered=1,
            metadata={"key": "value"}
        )
        
        assert state.agent_name == "test_agent"
        assert state.current_step_id == "step_123"
        assert state.is_active is True
        assert state.last_response == "Test response"
        assert isinstance(state.last_response_timestamp, datetime)
        assert state.tool_calls_made == 3
        assert state.tokens_used == 150
        assert state.errors_encountered == 1
        assert state.metadata == {"key": "value"}
    
    def test_agent_state_defaults(self):
        """AgentState should have sensible defaults."""
        state = AgentState(agent_name="test")
        
        assert state.agent_name == "test"
        assert state.current_step_id is None
        assert state.is_active is False
        assert state.last_response is None
        assert state.last_response_timestamp is None
        assert state.tool_calls_made == 0
        assert state.tokens_used == 0
        assert state.errors_encountered == 0
        assert state.metadata == {}


class TestAgentToolIntegration:
    """Test Agent tool integration."""
    
    def setup_method(self):
        """Setup test environment."""
        self.config = AgentConfig(
            name="test_agent",
            description="Test agent",
            tools=["search_web", "file_write"]
        )
        self.mock_tool_manager = Mock()
        self.mock_tool_manager.get_builtin_tools.return_value = ["file_read", "file_write"]
        self.mock_tool_manager.list_tools.return_value = ["file_read", "file_write", "search_web"]
        self.mock_tool_manager.get_tool_schemas.return_value = [
            {
                "type": "function",
                "function": {
                    "name": "search_web",
                    "description": "Search the web",
                    "parameters": {"type": "object", "properties": {}}
                }
            }
        ]
        
        self.agent = Agent(self.config, tool_manager=self.mock_tool_manager)
    
    def test_get_tools_json_returns_builtin_and_custom_tools(self):
        """get_tools_json should return schemas for builtin and custom tools."""
        tool_schemas = self.agent.get_tools_json()
        
        # Should call tool manager to get builtin tools
        self.mock_tool_manager.get_builtin_tools.assert_called_once()
        
        # Should call tool manager to get schemas for all tools
        self.mock_tool_manager.get_tool_schemas.assert_called_once()
        
        assert tool_schemas == [
            {
                "type": "function",
                "function": {
                    "name": "search_web",
                    "description": "Search the web",
                    "parameters": {"type": "object", "properties": {}}
                }
            }
        ]
    
    def test_get_tools_json_with_no_tool_manager(self):
        """get_tools_json should return empty list if no tool manager."""
        config = AgentConfig(name="test", description="Test")
        agent = Agent(config)
        
        tool_schemas = agent.get_tools_json()
        
        assert tool_schemas == []
    
    def test_get_tools_json_filters_unregistered_tools(self):
        """get_tools_json should only include actually registered tools."""
        config = AgentConfig(
            name="test",
            description="Test",
            tools=["search_web", "nonexistent_tool"]
        )
        
        # Mock tool manager that doesn't have nonexistent_tool
        mock_tool_manager = Mock()
        mock_tool_manager.get_builtin_tools.return_value = ["file_read"]
        mock_tool_manager.list_tools.return_value = ["file_read", "search_web"]  # No nonexistent_tool
        mock_tool_manager.get_tool_schemas.return_value = []
        
        agent = Agent(config, tool_manager=mock_tool_manager)
        
        # Should only include existing tools
        agent.get_tools_json()
        
        # Should not include nonexistent_tool in the final list
        expected_tools = ["file_read", "search_web"]
        mock_tool_manager.get_tool_schemas.assert_called_with(expected_tools)


class TestAgentResponseGeneration:
    """Test Agent response generation."""
    
    def setup_method(self):
        """Setup test environment."""
        self.config = AgentConfig(
            name="test_agent",
            description="Test agent",
            brain_config=BrainConfig(provider="openai", model="gpt-4")
        )
        self.agent = Agent(self.config)
    
    @pytest.mark.asyncio
    async def test_generate_response_basic(self):
        """generate_response should handle basic conversation."""
        messages = [{"role": "user", "content": "Hello"}]
        
        # Mock brain response
        mock_response = BrainResponse(
            content="Hello! How can I help you?",
            model="gpt-4",
            finish_reason="stop",
            timestamp=datetime.now()
        )
        
        # Mock both the brain response and the streaming loop
        with patch.object(self.agent.brain, 'generate_response') as mock_brain, \
             patch.object(self.agent, '_streaming_loop') as mock_streaming:
            
            mock_brain.return_value = mock_response
            
            # Mock the streaming loop to return simple text
            async def mock_stream_gen():
                yield {"type": "content", "content": "Hello! How can I help you?"}
            
            mock_streaming.return_value = mock_stream_gen()
            
            response = await self.agent.generate_response(messages)
            
            assert response == "Hello! How can I help you?"
            assert self.agent.state.is_active is False  # Should be inactive after completion
    
    @pytest.mark.asyncio
    async def test_generate_response_with_system_prompt(self):
        """generate_response should pass system prompt to brain."""
        messages = [{"role": "user", "content": "Test"}]
        system_prompt = "You are a helpful assistant."
        
        mock_response = BrainResponse(
            content="I'm here to help!",
            model="gpt-4",
            finish_reason="stop",
            timestamp=datetime.now()
        )
        
        # Mock both the brain response and the streaming loop
        with patch.object(self.agent.brain, 'generate_response') as mock_brain, \
             patch.object(self.agent, '_streaming_loop') as mock_streaming:
            
            mock_brain.return_value = mock_response
            
            # Mock the streaming loop to return the expected content
            async def mock_stream_gen():
                yield {"type": "content", "content": "I'm here to help!"}
            
            mock_streaming.return_value = mock_stream_gen()
            
            response = await self.agent.generate_response(messages, system_prompt=system_prompt)
            
            assert response == "I'm here to help!"
            # Should pass system prompt to streaming loop
            call_args = mock_streaming.call_args[0]
            assert call_args[1] == system_prompt  # system_prompt is second argument
    
    @pytest.mark.asyncio
    async def test_generate_response_with_orchestrator(self):
        """generate_response should work with orchestrator for tool execution."""
        messages = [{"role": "user", "content": "Search for Python tutorials"}]
        mock_orchestrator = Mock()
        
        mock_response = BrainResponse(
            content="I found some great Python tutorials!",
            model="gpt-4",
            finish_reason="stop",
            timestamp=datetime.now()
        )
        
        # Mock both the brain response and the streaming loop
        with patch.object(self.agent.brain, 'generate_response') as mock_brain, \
             patch.object(self.agent, '_streaming_loop') as mock_streaming:
            
            mock_brain.return_value = mock_response
            
            # Mock the streaming loop to return the expected content
            async def mock_stream_gen():
                yield {"type": "content", "content": "I found some great Python tutorials!"}
            
            mock_streaming.return_value = mock_stream_gen()
            
            response = await self.agent.generate_response(
                messages, 
                orchestrator=mock_orchestrator
            )
            
            assert response == "I found some great Python tutorials!"
            # Should pass orchestrator to streaming loop
            call_args = mock_streaming.call_args[0]
            assert call_args[2] is mock_orchestrator  # orchestrator is third argument
    
    @pytest.mark.asyncio
    async def test_generate_response_state_management(self):
        """generate_response should manage agent state correctly."""
        messages = [{"role": "user", "content": "Test"}]
        
        mock_response = BrainResponse(
            content="Response",
            model="gpt-4",
            finish_reason="stop",
            timestamp=datetime.now()
        )
        
        # Mock both the brain response and the streaming loop
        with patch.object(self.agent.brain, 'generate_response') as mock_brain, \
             patch.object(self.agent, '_streaming_loop') as mock_streaming:
            
            mock_brain.return_value = mock_response
            
            # Mock the streaming loop to return the expected content
            async def mock_stream_gen():
                yield {"type": "content", "content": "Response"}
            
            mock_streaming.return_value = mock_stream_gen()
            
            # State should be inactive before
            assert self.agent.state.is_active is False
            
            response = await self.agent.generate_response(messages)
            
            # State should be inactive after completion
            assert self.agent.state.is_active is False
            assert response == "Response"
    
    @pytest.mark.asyncio
    async def test_generate_response_handles_brain_errors(self):
        """generate_response should handle brain errors gracefully."""
        messages = [{"role": "user", "content": "Test"}]
        
        # Mock the streaming loop to raise an exception
        with patch.object(self.agent, '_streaming_loop') as mock_streaming:
            
            async def mock_stream_gen():
                raise Exception("Brain error")
                yield  # This never executes but makes it a generator
            
            mock_streaming.return_value = mock_stream_gen()
            
            # Should handle error gracefully by raising it (not swallowing it)
            with pytest.raises(Exception, match="Brain error"):
                await self.agent.generate_response(messages)
            
            # State should be inactive after error
            assert self.agent.state.is_active is False
    
    @pytest.mark.asyncio
    async def test_generate_response_with_non_streaming_brain(self):
        """generate_response should handle non-streaming brain configuration."""
        # Configure brain for non-streaming
        brain_config = BrainConfig(
            provider="openai",
            model="gpt-4",
            streaming=False
        )
        config = AgentConfig(
            name="test_agent",
            description="Test",
            brain_config=brain_config
        )
        agent = Agent(config)
        
        messages = [{"role": "user", "content": "Test"}]
        
        mock_response = BrainResponse(
            content="Non-streaming response",
            model="gpt-4",
            finish_reason="stop",
            timestamp=datetime.now()
        )
        
        with patch.object(agent.brain, 'generate_response') as mock_brain:
            mock_brain.return_value = mock_response
            
            response = await agent.generate_response(messages)
            
            assert response == "Non-streaming response"
            mock_brain.assert_called_once()


class TestAgentStreamingResponse:
    """Test Agent streaming response functionality."""
    
    def setup_method(self):
        """Setup test environment."""
        self.config = AgentConfig(
            name="test_agent",
            description="Test agent",
            brain_config=BrainConfig(provider="openai", model="gpt-4")
        )
        self.agent = Agent(self.config)
    
    @pytest.mark.asyncio
    async def test_stream_response_yields_chunks(self):
        """stream_response should yield chunks from brain."""
        messages = [{"role": "user", "content": "Tell me a story"}]
        
        # Mock the streaming loop directly
        async def mock_stream_gen():
            yield {"type": "content", "content": "Once"}
            yield {"type": "content", "content": " upon"}
            yield {"type": "content", "content": " a time"}
            yield {"type": "finish", "finish_reason": "stop"}
        
        with patch.object(self.agent, '_streaming_loop') as mock_streaming:
            mock_streaming.return_value = mock_stream_gen()
            
            chunks = []
            async for chunk in self.agent.stream_response(messages):
                chunks.append(chunk)
            
            assert len(chunks) == 4
            assert chunks[0]["content"] == "Once"
            assert chunks[1]["content"] == " upon"
            assert chunks[2]["content"] == " a time"
            assert chunks[3]["finish_reason"] == "stop"
    
    @pytest.mark.asyncio
    async def test_stream_response_handles_tool_calls(self):
        """stream_response should handle tool call chunks."""
        messages = [{"role": "user", "content": "Search for something"}]
        
        # Mock the streaming loop to emit tool-related chunks
        async def mock_stream_gen():
            yield {"type": "tool-call", "tool_call": {"id": "call_123", "function": {"name": "search_web"}}}
            yield {"type": "tool-result", "tool_call_id": "call_123", "result": "Search results"}
            yield {"type": "content", "content": "Here are the results"}
            yield {"type": "finish", "finish_reason": "stop"}
        
        with patch.object(self.agent, '_streaming_loop') as mock_streaming:
            mock_streaming.return_value = mock_stream_gen()
            
            chunks = []
            async for chunk in self.agent.stream_response(messages):
                chunks.append(chunk)
            
            assert len(chunks) == 4
            assert chunks[0]["type"] == "tool-call"
            assert chunks[1]["type"] == "tool-result"
            assert chunks[2]["type"] == "content"
            assert chunks[3]["type"] == "finish"
    
    @pytest.mark.asyncio
    async def test_stream_response_with_orchestrator(self):
        """stream_response should work with orchestrator."""
        messages = [{"role": "user", "content": "Test"}]
        mock_orchestrator = Mock()
        
        # Mock the streaming loop
        async def mock_stream_gen():
            yield {"type": "content", "content": "Test response"}
            yield {"type": "finish", "finish_reason": "stop"}
        
        with patch.object(self.agent, '_streaming_loop') as mock_streaming:
            mock_streaming.return_value = mock_stream_gen()
            
            chunks = []
            async for chunk in self.agent.stream_response(messages, orchestrator=mock_orchestrator):
                chunks.append(chunk)
            
            assert len(chunks) == 2
            # Should pass orchestrator to streaming loop
            call_args = mock_streaming.call_args[0]
            assert call_args[2] is mock_orchestrator  # orchestrator is third argument
    
    @pytest.mark.asyncio
    async def test_stream_response_state_management(self):
        """stream_response should manage agent state during streaming."""
        messages = [{"role": "user", "content": "Test"}]
        
        # Mock the streaming loop
        async def mock_stream_gen():
            yield {"type": "content", "content": "Test"}
            yield {"type": "finish", "finish_reason": "stop"}
        
        with patch.object(self.agent, '_streaming_loop') as mock_streaming:
            mock_streaming.return_value = mock_stream_gen()
            
            chunks = []
            async for chunk in self.agent.stream_response(messages):
                chunks.append(chunk)
            
            # Should manage state correctly
            assert self.agent.state.is_active is False  # Should be inactive after completion


class TestAgentIntegration:
    """Test Agent integration scenarios."""
    
    def setup_method(self):
        """Setup test environment."""
        self.brain_config = BrainConfig(
            provider="deepseek",
            model="deepseek-chat",
            temperature=0.7
        )
        self.config = AgentConfig(
            name="integration_agent",
            description="Integration test agent",
            brain_config=self.brain_config,
            tools=["search_web", "file_write"],
            memory_enabled=True,
            max_iterations=5
        )
        
        self.mock_tool_manager = Mock()
        self.mock_tool_manager.get_builtin_tools.return_value = ["file_read", "file_write"]
        self.mock_tool_manager.list_tools.return_value = ["file_read", "file_write", "search_web"]
        self.mock_tool_manager.get_tool_schemas.return_value = []
        
        self.agent = Agent(self.config, tool_manager=self.mock_tool_manager)
    
    def test_agent_configuration_consistency(self):
        """Agent should maintain configuration consistency."""
        assert self.agent.name == "integration_agent"
        assert self.agent.description == "Integration test agent"
        assert self.agent.tools == ["search_web", "file_write"]
        assert self.agent.memory_enabled is True
        assert self.agent.max_iterations == 5
        
        # Brain should use the configured settings
        assert self.agent.brain.config is self.brain_config
        assert self.agent.brain.config.provider == "deepseek"
        assert self.agent.brain.config.model == "deepseek-chat"
        assert self.agent.brain.config.temperature == 0.7
    
    def test_agent_tool_manager_integration(self):
        """Agent should integrate properly with tool manager."""
        # Should use injected tool manager
        assert self.agent.tool_manager is self.mock_tool_manager
        
        # Should get tool schemas properly
        tool_schemas = self.agent.get_tools_json()
        
        # Should call tool manager methods
        self.mock_tool_manager.get_builtin_tools.assert_called_once()
        self.mock_tool_manager.get_tool_schemas.assert_called_once()
    
    @pytest.mark.asyncio
    async def test_agent_full_conversation_flow(self):
        """Test complete conversation flow with Agent."""
        messages = [
            {"role": "user", "content": "Hello"},
            {"role": "assistant", "content": "Hi there!"},
            {"role": "user", "content": "Can you help me search for information?"}
        ]
        
        mock_response = BrainResponse(
            content="I can help you search for information! What would you like to find?",
            model="deepseek-chat",
            finish_reason="stop",
            timestamp=datetime.now()
        )
        
        # Mock both the brain response and the streaming loop
        with patch.object(self.agent.brain, 'generate_response') as mock_brain, \
             patch.object(self.agent, '_streaming_loop') as mock_streaming:
            
            mock_brain.return_value = mock_response
            
            # Mock the streaming loop to return the expected content
            async def mock_stream_gen():
                yield {"type": "content", "content": "I can help you search for information! What would you like to find?"}
            
            mock_streaming.return_value = mock_stream_gen()
            
            response = await self.agent.generate_response(messages)
            
            assert response == "I can help you search for information! What would you like to find?"
            
            # Should pass all messages to streaming loop
            call_args = mock_streaming.call_args[0]
            assert call_args[0] == messages  # messages is first argument
            
            # Should use default parameters
            assert call_args[1] is None  # system_prompt is None
            assert call_args[2] is None  # orchestrator is None
            assert call_args[3] == 10  # max_tool_rounds is 10
    
    @pytest.mark.asyncio
    async def test_agent_with_system_prompt_override(self):
        """Test agent with system prompt override."""
        messages = [{"role": "user", "content": "Test"}]
        custom_system_prompt = "You are a specialized search assistant."
        
        mock_response = BrainResponse(
            content="I'm a specialized search assistant!",
            model="deepseek-chat",
            finish_reason="stop",
            timestamp=datetime.now()
        )
        
        # Mock both the brain response and the streaming loop
        with patch.object(self.agent.brain, 'generate_response') as mock_brain, \
             patch.object(self.agent, '_streaming_loop') as mock_streaming:
            
            mock_brain.return_value = mock_response
            
            # Mock the streaming loop to return the expected content
            async def mock_stream_gen():
                yield {"type": "content", "content": "I'm a specialized search assistant!"}
            
            mock_streaming.return_value = mock_stream_gen()
            
            response = await self.agent.generate_response(
                messages, 
                system_prompt=custom_system_prompt
            )
            
            assert response == "I'm a specialized search assistant!"
            
            # Should pass custom system prompt to streaming loop
            call_args = mock_streaming.call_args[0]
            assert call_args[1] == custom_system_prompt  # system_prompt is second argument
    
    def test_agent_handles_missing_optional_config(self):
        """Agent should handle missing optional configuration gracefully."""
        minimal_config = AgentConfig(
            name="minimal_agent",
            description="Minimal configuration"
        )
        
        agent = Agent(minimal_config)
        
        # Should use defaults
        assert agent.name == "minimal_agent"
        assert agent.description == "Minimal configuration"
        assert agent.tools == []  # Default empty tools
        assert agent.memory_enabled is True  # Default enabled
        assert agent.max_iterations == 10  # Default max iterations
        assert isinstance(agent.brain, Brain)
        assert agent.brain.config.provider == "deepseek"  # Default provider
    
    def test_agent_string_representation(self):
        """Agent should have useful string representation."""
        agent_str = str(self.agent)
        
        # Should include key agent information
        assert "integration_agent" in agent_str or hasattr(self.agent, '__str__')
        # Note: The actual Agent class might not have __str__ implemented,
        # but we test that it at least has a default representation 