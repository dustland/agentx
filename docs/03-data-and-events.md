# 03: Data Structures & Event Model

This document specifies the core data objects that flow through the AgentX framework and the event model used for real-time observability. It builds on the architecture and collaboration models defined in the previous documents.

## 1. The `TaskStep` Object

**(Req #12)** The `TaskStep` is the fundamental unit of history. It represents a single, complete turn taken by an entity (an agent or a tool executor) in the system.

- The `TaskStep` is designed to be extensible and support multimodal data through its `parts` structure.
- **(Req #7)** All `TaskStep` objects are persisted to `history.jsonl` in the Task Workspace for full auditability.
- **(Req #20)** TaskSteps include memory context and cross-references for enhanced collaboration.

**Schema:**

```python
class TaskStep(BaseModel):
    step_id: str = Field(default_factory=lambda: f"step_{uuid.uuid4().hex}")
    timestamp: datetime = Field(default_factory=datetime.utcnow)
    agent_name: str  # The agent or system component that generated this step
    task_id: Optional[str] = None  # Reference to plan.json task
    parent_step_id: Optional[str] = None  # For hierarchical execution
    execution_mode: str = "autonomous"  # autonomous, step, hitl, etc.
    memory_context: Dict[str, Any] = {}  # Memory references and context
    parts: List[Union[TextPart, ToolCallPart, ToolResultPart, ArtifactPart, ImagePart, AudioPart, MemoryPart, GuardrailPart]]
    metadata: Dict[str, Any] = {}  # Extensible metadata
```

## 2. `TaskStep` Parts

**(Req #12)** A `TaskStep` is composed of one or more `Part`s, each representing a different kind of data. The part system is designed to be multimodal-ready and extensible.

### 2.1. `TextPart`

Contains text generated by an agent or user.

```python
class TextPart(BaseModel):
    type: Literal["text"] = "text"
    text: str
    language: Optional[str] = None  # For multilingual support
    confidence: Optional[float] = None  # LLM confidence score
```

### 2.2. `ToolCallPart`

A structured request from an agent to call a tool.

```python
class ToolCall(BaseModel):
    id: str = Field(default_factory=lambda: f"tc_{uuid.uuid4().hex}")
    tool_name: str
    args: Dict[str, Any]
    expected_output_type: Optional[str] = None
    timeout: Optional[int] = None
    retry_policy: Optional[Dict[str, Any]] = None

class ToolCallPart(BaseModel):
    type: Literal["tool_call"] = "tool_call"
    tool_call: ToolCall
```

### 2.3. `ToolResultPart`

The output from the `ToolExecutor` after running a tool. It is explicitly linked to a `ToolCall` by its `tool_call_id`.

```python
class ToolResult(BaseModel):
    tool_call_id: str
    result: str # The stdout or serialized result from the tool
    error: Optional[str] = None # Stderr if the tool failed
    artifacts: List[str] = [] # Paths to any files created by the tool
    execution_time_ms: Optional[int] = None
    resource_usage: Optional[Dict[str, Any]] = None  # CPU, memory, etc.
    exit_code: Optional[int] = None

class ToolResultPart(BaseModel):
    type: Literal["tool_result"] = "tool_result"
    tool_result: ToolResult
```

### 2.4. `ArtifactPart`

A reference to a file in the `Task Workspace`. This allows the history to remain lightweight while handling large data like generated code, images, or documents.

```python
class Artifact(BaseModel):
    uri: str  # e.g., "file://artifacts/main.py"
    mime_type: str
    size_bytes: Optional[int] = None
    description: Optional[str] = None
    version: Optional[str] = None  # For artifact versioning
    checksum: Optional[str] = None  # For integrity verification
    metadata: Dict[str, Any] = {}  # Extensible metadata
    created_by: Optional[str] = None  # Agent or tool that created it
    tags: List[str] = []  # For categorization and search

class ArtifactPart(BaseModel):
    type: Literal["artifact"] = "artifact"
    artifact: Artifact
```

### 2.5. Multimodal Parts

**(Req #12)** Support for multimodal data:

```python
class ImagePart(BaseModel):
    type: Literal["image"] = "image"
    image_url: str  # Can be data URL or artifact reference
    alt_text: Optional[str] = None
    dimensions: Optional[Dict[str, int]] = None  # width, height
    format: Optional[str] = None  # png, jpg, etc.

class AudioPart(BaseModel):
    type: Literal["audio"] = "audio"
    audio_url: str  # Can be data URL or artifact reference
    transcript: Optional[str] = None
    duration_seconds: Optional[float] = None
    format: Optional[str] = None  # mp3, wav, etc.
    sample_rate: Optional[int] = None
```

### 2.6. Memory Parts

**(Req #20)** Memory system integration:

```python
class MemoryReference(BaseModel):
    memory_id: str
    memory_type: str  # "short_term", "long_term", "semantic", "episodic"
    relevance_score: Optional[float] = None
    retrieval_query: Optional[str] = None

class MemoryPart(BaseModel):
    type: Literal["memory"] = "memory"
    operation: str  # "store", "retrieve", "search", "consolidate"
    references: List[MemoryReference]
    content: Optional[Dict[str, Any]] = None
```

### 2.7. Guardrail Parts

**(Req #21)** Safety and compliance tracking:

```python
class GuardrailCheck(BaseModel):
    check_id: str
    check_type: str  # "input_validation", "content_filter", "rate_limit", "policy"
    status: str  # "passed", "failed", "warning"
    message: Optional[str] = None
    policy_violated: Optional[str] = None
    severity: Optional[str] = None  # "low", "medium", "high", "critical"

class GuardrailPart(BaseModel):
    type: Literal["guardrail"] = "guardrail"
    checks: List[GuardrailCheck]
    overall_status: str  # "passed", "failed", "warning"
```

## 3. Real-time Client Updates (Dual Channel)

**(Req #10)** To provide a rich, observable, and UI-friendly experience, the `Orchestrator` produces two distinct streams of information for the client. This model ensures clients can get both low-latency message streaming and high-level structured notifications about the task's lifecycle.

### 3.1. Channel 1: Message Streaming (`StreamChunk`)

This channel provides real-time, token-by-token output directly from the LLM. It is designed for low-latency UI updates, enabling a "typing" effect.

**Important:** "Streaming" refers exclusively to token-by-token LLM output, not to the sending of discrete event notifications.

**Schema:**

```python
class StreamChunk(BaseModel):
    type: Literal["content_chunk"] = "content_chunk"
    step_id: str  # Links to the TaskStep being generated
    agent_name: str
    text: str
    is_final: bool = False  # True for the last chunk of a response
    token_count: Optional[int] = None
    timestamp: datetime = Field(default_factory=datetime.utcnow)
```

As the `Orchestrator` receives tokens from the agent's "Brain", it immediately forwards them on this stream.

### 3.2. Channel 2: Execution Notifications (`ExecutionEvent`)

This channel provides structured notifications about significant lifecycle events within the `Orchestrator`. These are used for building robust UIs, logging, and debugging.

**Schema (Comprehensive Event Types):**

```python
# Task Lifecycle Events
class TaskStartEvent(BaseModel):
    type: Literal["event_task_start"] = "event_task_start"
    task_id: str
    timestamp: datetime
    initial_prompt: str
    execution_mode: str
    team_config: Dict[str, Any]

class TaskCompleteEvent(BaseModel):
    type: Literal["event_task_complete"] = "event_task_complete"
    task_id: str
    timestamp: datetime
    final_status: Literal["success", "error", "cancelled"]
    summary: Optional[str] = None
    artifacts_created: List[str] = []
    total_steps: int
    total_duration_ms: int

class TaskPausedEvent(BaseModel):
    type: Literal["event_task_paused"] = "event_task_paused"
    task_id: str
    timestamp: datetime
    reason: str  # "step_mode", "breakpoint", "user_request", "hitl_intervention"
    context: Dict[str, Any] = {}

class TaskResumedEvent(BaseModel):
    type: Literal["event_task_resumed"] = "event_task_resumed"
    task_id: str
    timestamp: datetime
    reason: str
    context: Dict[str, Any] = {}

# Agent Events
class AgentStartEvent(BaseModel):
    type: Literal["event_agent_start"] = "event_agent_start"
    agent_name: str
    step_id: str
    timestamp: datetime
    context_size: Optional[int] = None
    memory_retrieved: Optional[int] = None

class AgentCompleteEvent(BaseModel):
    type: Literal["event_agent_complete"] = "event_agent_complete"
    agent_name: str
    step_id: str
    timestamp: datetime
    token_count: Optional[int] = None
    execution_time_ms: Optional[int] = None
    memory_stored: Optional[int] = None

class AgentHandoffEvent(BaseModel):
    type: Literal["event_agent_handoff"] = "event_agent_handoff"
    from_agent: str
    to_agent: str
    reason: str
    timestamp: datetime
    context: Dict[str, Any] = {}
    handoff_type: str = "sequential"  # "sequential", "parallel", "hierarchical"

# Advanced Collaboration Events
class ParallelExecutionStartEvent(BaseModel):
    type: Literal["event_parallel_start"] = "event_parallel_start"
    agents: List[str]
    coordination_agent: Optional[str] = None
    timestamp: datetime
    sync_points: List[str] = []

class ParallelExecutionSyncEvent(BaseModel):
    type: Literal["event_parallel_sync"] = "event_parallel_sync"
    sync_point: str
    completed_agents: List[str]
    waiting_agents: List[str]
    timestamp: datetime

class ConsensusProposalEvent(BaseModel):
    type: Literal["event_consensus_proposal"] = "event_consensus_proposal"
    proposal_id: str
    proposer_agent: str
    decision: str
    stakeholders: List[str]
    timestamp: datetime

class ConsensusVoteEvent(BaseModel):
    type: Literal["event_consensus_vote"] = "event_consensus_vote"
    proposal_id: str
    voter_agent: str
    vote: str
    reasoning: str
    timestamp: datetime

class ConsensusReachedEvent(BaseModel):
    type: Literal["event_consensus_reached"] = "event_consensus_reached"
    proposal_id: str
    final_decision: str
    votes: Dict[str, str]
    timestamp: datetime

# Tool Events
class ToolCallEvent(BaseModel):
    type: Literal["event_tool_call"] = "event_tool_call"
    tool_call: ToolCall
    agent_name: str
    timestamp: datetime
    sandbox_id: Optional[str] = None

class ToolResultEvent(BaseModel):
    type: Literal["event_tool_result"] = "event_tool_result"
    tool_result: ToolResult
    timestamp: datetime
    execution_time_ms: Optional[int] = None
    sandbox_id: Optional[str] = None

# Memory Events
class MemoryStoreEvent(BaseModel):
    type: Literal["event_memory_store"] = "event_memory_store"
    memory_id: str
    memory_type: str
    agent_name: str
    content_size: int
    timestamp: datetime

class MemoryRetrieveEvent(BaseModel):
    type: Literal["event_memory_retrieve"] = "event_memory_retrieve"
    query: str
    results_count: int
    agent_name: str
    timestamp: datetime
    relevance_threshold: Optional[float] = None

class MemoryConsolidateEvent(BaseModel):
    type: Literal["event_memory_consolidate"] = "event_memory_consolidate"
    topic: str
    items_consolidated: int
    timestamp: datetime
    summary_length: int

# HITL Events
class HITLRequestEvent(BaseModel):
    type: Literal["event_hitl_request"] = "event_hitl_request"
    request_id: str
    request_type: str  # "approval", "feedback", "escalation"
    agent_name: str
    context: Dict[str, Any]
    timeout: Optional[int] = None
    timestamp: datetime

class HITLResponseEvent(BaseModel):
    type: Literal["event_hitl_response"] = "event_hitl_response"
    request_id: str
    response_type: str  # "approved", "rejected", "modified", "timeout"
    response_data: Dict[str, Any]
    response_time_ms: int
    timestamp: datetime

# Guardrail Events
class GuardrailViolationEvent(BaseModel):
    type: Literal["event_guardrail_violation"] = "event_guardrail_violation"
    violation_id: str
    check_type: str
    severity: str
    policy_violated: str
    agent_name: Optional[str] = None
    content_sample: Optional[str] = None
    action_taken: str  # "blocked", "warned", "logged"
    timestamp: datetime

class GuardrailPolicyUpdateEvent(BaseModel):
    type: Literal["event_guardrail_policy_update"] = "event_guardrail_policy_update"
    policy_name: str
    update_type: str  # "created", "modified", "deleted"
    updated_by: str
    timestamp: datetime

# Artifact Events
class ArtifactCreatedEvent(BaseModel):
    type: Literal["event_artifact_created"] = "event_artifact_created"
    artifact: Artifact
    created_by: str
    timestamp: datetime

class ArtifactModifiedEvent(BaseModel):
    type: Literal["event_artifact_modified"] = "event_artifact_modified"
    artifact_uri: str
    modified_by: str
    changes: Dict[str, Any]
    version: str
    timestamp: datetime

class ArtifactVersionedEvent(BaseModel):
    type: Literal["event_artifact_versioned"] = "event_artifact_versioned"
    artifact_uri: str
    old_version: str
    new_version: str
    reason: str
    timestamp: datetime

# Error Events
class ErrorEvent(BaseModel):
    type: Literal["event_error"] = "event_error"
    error_id: str
    error_type: str
    error_message: str
    context: Dict[str, Any]
    timestamp: datetime
    recoverable: bool
    recovery_action: Optional[str] = None
    stack_trace: Optional[str] = None

class RecoveryEvent(BaseModel):
    type: Literal["event_recovery"] = "event_recovery"
    error_id: str
    recovery_strategy: str
    success: bool
    timestamp: datetime
    details: Dict[str, Any] = {}

# Step-Through Events
class BreakpointHitEvent(BaseModel):
    type: Literal["event_breakpoint_hit"] = "event_breakpoint_hit"
    breakpoint_id: str
    breakpoint_type: str
    context: Dict[str, Any]
    timestamp: datetime
    agent_name: Optional[str] = None

class UserInterventionEvent(BaseModel):
    type: Literal["event_user_intervention"] = "event_user_intervention"
    intervention_id: str
    intervention_type: str  # "instruction", "plan_update", "agent_override", "team_modification"
    details: Dict[str, Any]
    timestamp: datetime
    user_id: Optional[str] = None

# System Health Events
class HealthCheckEvent(BaseModel):
    type: Literal["event_health_check"] = "event_health_check"
    component: str
    status: str  # "healthy", "degraded", "unhealthy"
    metrics: Dict[str, Any]
    timestamp: datetime

class PerformanceMetricEvent(BaseModel):
    type: Literal["event_performance_metric"] = "event_performance_metric"
    metric_name: str
    metric_value: float
    metric_unit: str
    component: str
    timestamp: datetime
    tags: Dict[str, str] = {}
```

The `Orchestrator` sends these notification objects as it performs actions like starting a task, processing a handoff, dispatching a tool call, or receiving a result. This gives the client a complete, real-time picture of the task's state.

## 4. Configuration Data Structures

**(Req #1, #4, #8, #17)** File-based configuration structures:

### 4.1. Team Configuration

```python
class LLMConfig(BaseModel):
    provider: str = "deepseek"  # Default provider (Req #17)
    model: str = "deepseek-chat"  # Default model (Req #17)
    temperature: float = 0.7
    max_tokens: Optional[int] = None
    api_key: Optional[str] = None
    base_url: Optional[str] = None
    timeout: int = 30
    retry_policy: Dict[str, Any] = {}

class ToolConfig(BaseModel):
    name: str
    type: Literal["builtin", "custom", "mcp", "hitl"]
    config: Dict[str, Any] = {}
    # For custom tools
    path: Optional[str] = None
    function: Optional[str] = None
    # For MCP tools
    server_url: Optional[str] = None
    # For HITL tools
    timeout: Optional[int] = None
    escalation_policy: Optional[str] = None

class HandoffRule(BaseModel):
    from_agent: str
    to_agent: str
    condition: Optional[str] = None  # Future: conditional handoffs
    handoff_type: str = "sequential"  # "sequential", "parallel", "hierarchical"

class CollaborationPattern(BaseModel):
    name: str
    type: str  # "parallel", "hierarchical", "consensus"
    agents: List[str]
    coordination_agent: Optional[str] = None
    config: Dict[str, Any] = {}

class GuardrailPolicy(BaseModel):
    name: str
    type: str  # "input_validation", "content_filter", "rate_limit", "compliance"
    rules: List[Dict[str, Any]]
    severity: str = "medium"
    action: str = "warn"  # "block", "warn", "log"

class MemoryConfig(BaseModel):
    short_term_limit: int = 10000  # tokens
    long_term_enabled: bool = True
    semantic_search_enabled: bool = True
    consolidation_interval: int = 3600  # seconds
    vector_db_config: Dict[str, Any] = {}

class AgentConfig(BaseModel):
    name: str
    role: str
    prompt_template: str  # Path to Jinja2 template file
    llm_config: Optional[LLMConfig] = None  # Override default LLM
    tools: List[str] = []  # Tool names available to this agent
    memory_config: Optional[MemoryConfig] = None
    guardrail_policies: List[str] = []
    collaboration_patterns: List[str] = []
    max_parallel_tasks: int = 1

class ExecutionConfig(BaseModel):
    mode: str = "autonomous"  # "autonomous", "step", "hitl"
    timeout: Optional[int] = None
    max_steps: Optional[int] = None
    breakpoints: List[str] = []
    human_intervention_points: List[str] = []
    success_criteria: List[str] = []
    failure_criteria: List[str] = []

class TeamConfig(BaseModel):
    name: str
    description: str
    version: str = "1.0.0"
    default_llm: LLMConfig
    agents: List[AgentConfig]
    tools: List[ToolConfig]
    handoff_rules: List[HandoffRule] = []
    collaboration_patterns: List[CollaborationPattern] = []
    guardrail_policies: List[GuardrailPolicy] = []
    memory_config: MemoryConfig = MemoryConfig()
    execution_config: ExecutionConfig = ExecutionConfig()
    deployment_config: Dict[str, Any] = {}
```

### 4.2. Task Workspace Structure

**(Req #7, #25)** The Task Workspace contains:

```
task_workspace/
├── team.yaml              # Team configuration
├── plan.json             # Structured execution plan
├── history.jsonl         # Append-only TaskStep log
├── artifacts/            # Generated files with versioning
│   ├── code/
│   │   ├── main.py
│   │   └── .versions/    # Version history
│   ├── documents/
│   │   ├── report.md
│   │   └── .metadata.json # Artifact metadata
│   ├── media/
│   └── data/
├── memory/               # Memory system storage
│   ├── short_term.json   # Recent context
│   ├── long_term/        # Persistent memory
│   │   ├── semantic.db   # Vector database
│   │   └── episodic.json # Event-based memory
│   └── consolidated/     # Summarized memory
├── .agentx/              # Framework metadata
│   ├── state.json        # Current execution state
│   ├── breakpoints.json  # Step-through configuration
│   ├── guardrails.json   # Policy violations and actions
│   ├── performance.json  # Metrics and health data
│   └── logs/             # Detailed execution logs
│       ├── orchestrator.log
│       ├── agents.log
│       ├── tools.log
│       └── errors.log
└── sandbox/              # Daytona sandbox workspace
    ├── workspace.yaml    # Sandbox configuration
    └── shared/           # Shared files with sandbox
```

## 5. API Interfaces

**(Req #14, #16, #24)** Backend-first design with UI-friendly APIs and production-grade features:

### 5.1. REST API Endpoints

```python
# Task Management
POST /tasks                    # Create new task
GET /tasks/{task_id}          # Get task status
POST /tasks/{task_id}/start   # Start task execution
POST /tasks/{task_id}/pause   # Pause execution
POST /tasks/{task_id}/resume  # Resume execution
DELETE /tasks/{task_id}       # Cancel/delete task

# Step-Through Control
POST /tasks/{task_id}/step    # Execute one step
POST /tasks/{task_id}/continue # Continue to next breakpoint
POST /tasks/{task_id}/intervene # Inject user instruction

# HITL Endpoints
GET /tasks/{task_id}/hitl/requests    # Get pending HITL requests
POST /tasks/{task_id}/hitl/{request_id}/respond # Respond to HITL request

# Memory Management
GET /tasks/{task_id}/memory           # Get memory summary
POST /tasks/{task_id}/memory/search   # Search memory
POST /tasks/{task_id}/memory/consolidate # Trigger consolidation

# Artifact Management
GET /tasks/{task_id}/artifacts        # List artifacts
GET /tasks/{task_id}/artifacts/{path} # Get artifact content
GET /tasks/{task_id}/artifacts/{path}/versions # Get version history
POST /tasks/{task_id}/artifacts/{path}/restore/{version} # Restore version

# Data Access
GET /tasks/{task_id}/history  # Get task history
GET /tasks/{task_id}/plan     # Get execution plan
POST /tasks/{task_id}/plan    # Update execution plan

# Health and Monitoring
GET /health                   # System health check
GET /metrics                  # Performance metrics
GET /tasks/{task_id}/metrics  # Task-specific metrics

# Team Management
GET /teams                    # List available teams
POST /teams                   # Create new team
GET /teams/{team_id}          # Get team configuration
PUT /teams/{team_id}          # Update team configuration

# Framework Compatibility
POST /import/langchain        # Import LangChain configuration
POST /import/autogen          # Import AutoGen configuration
POST /import/crewai           # Import CrewAI configuration
```

### 5.2. WebSocket Streams

```python
# Dual-channel streaming
WS /tasks/{task_id}/stream    # Combined stream (both channels)
WS /tasks/{task_id}/content   # Content streaming only
WS /tasks/{task_id}/events    # Execution events only

# Specialized streams
WS /tasks/{task_id}/memory    # Memory operations stream
WS /tasks/{task_id}/guardrails # Guardrail events stream
WS /tasks/{task_id}/artifacts  # Artifact changes stream
WS /tasks/{task_id}/hitl      # HITL requests and responses
```

### 5.3. Production Features

**(Req #24)** Production-grade API features:

- **Authentication**: JWT-based authentication with role-based access control
- **Rate Limiting**: Configurable rate limits per endpoint and user
- **Circuit Breakers**: Automatic failure protection with graceful degradation
- **Health Checks**: Comprehensive health monitoring with dependency checks
- **Metrics**: Prometheus-compatible metrics for monitoring and alerting
- **Logging**: Structured logging with correlation IDs and distributed tracing
- **Caching**: Redis-based caching for frequently accessed data
- **Load Balancing**: Support for horizontal scaling with session affinity

## 6. Cross-Framework Data Compatibility

**(Req #22)** Data structures for framework compatibility:

### 6.1. LangChain Compatibility

```python
class LangChainImport(BaseModel):
    chain_type: str
    chain_config: Dict[str, Any]
    agents: List[Dict[str, Any]]
    tools: List[Dict[str, Any]]
    memory_config: Optional[Dict[str, Any]] = None

class LangChainExport(BaseModel):
    agentx_team: TeamConfig
    langchain_equivalent: Dict[str, Any]
    migration_notes: List[str]
```

### 6.2. AutoGen/AG2 Compatibility

```python
class AutoGenImport(BaseModel):
    group_chat_config: Dict[str, Any]
    agents: List[Dict[str, Any]]
    conversation_patterns: List[Dict[str, Any]]

class AutoGenExport(BaseModel):
    agentx_team: TeamConfig
    autogen_equivalent: Dict[str, Any]
    pattern_mappings: Dict[str, str]
```

### 6.3. CrewAI Compatibility

```python
class CrewAIImport(BaseModel):
    crew_config: Dict[str, Any]
    agents: List[Dict[str, Any]]
    tasks: List[Dict[str, Any]]
    tools: List[Dict[str, Any]]

class CrewAIExport(BaseModel):
    agentx_team: TeamConfig
    crewai_equivalent: Dict[str, Any]
    role_mappings: Dict[str, str]
```

## 7. Design Complete

With this document, the full design specification for the AgentX framework is complete. We have defined:

1.  The high-level **Architecture** with all component responsibilities including new advanced features.
2.  The core **Collaboration & Planning Model** with universal tool support, HITL capabilities, and advanced collaboration patterns.
3.  The detailed **Data Structures & Event Model** with multimodal support, comprehensive event types, production-grade features, and cross-framework compatibility.

This provides a comprehensive blueprint that addresses all 26 requirements for implementation, positioning AgentX to supersede existing frameworks like AG2, Suna, Manus, OpenAI Agents SDK, and Vercel AI SDK v5.
