# Design Doc: Data and Memory

This document defines how Roboco manages data, history, and memory. The primary goal is to create a system that is robust, scalable, and auditable, moving beyond the limitations of simple in-memory message lists.

## 1. The Task Workspace

Every task initiated by the `Orchestrator` will have a dedicated **Task Workspace** on the file system. This directory is the single source of truth for a given task run.

- **Location**: A configurable base directory (e.g., `/var/roboco/workspaces/`).
- **Structure**: Each workspace will have a unique ID and a standardized structure.
  ```
  /workspaces/
    └── {task_id_1}/
        ├── team.json
        ├── history.jsonl
        └── artifacts/
            ├── {artifact_id_1}.bin
            └── {artifact_id_2}.txt
  ```
- **`team.json`**: A snapshot of the configuration used to initiate this task.
- **`history.jsonl`**: A log of all `TaskStep`s, stored as a JSON Lines file for efficient appending.
- **`artifacts/`**: A directory for storing large binary or text data generated or used during the task.

## 2. The `TaskStep`: A Structured Unit of History

We will replace the simple concept of a "message" with a more structured and extensible object called `TaskStep`. A `TaskStep` represents a single, complete turn in the collaboration loop and is composed of multiple `Part`s.

```python
class TaskStep(BaseModel):
    step_id: str = Field(default_factory=lambda: f"step_{uuid.uuid4().hex}")
    agent_name: str
    parts: List[Union[TextPart, ToolCallPart, ToolResultPart, ArtifactPart]]
```

### `TaskStep` Parts

- **`TextPart`**: Contains text generated by an agent or user. This is the primary content of a turn.
- **`ToolCallPart`**: A structured representation of an agent's request to call a specific tool with certain arguments.
- **`ToolResultPart`**: The output generated by the execution of a tool. It is explicitly linked to a `ToolCallPart` by its `tool_call_id`.
- **`ArtifactPart`**: A reference to a piece of data stored in the `artifacts/` directory of the Task Workspace. This allows the history to remain lightweight while handling large data.

```python
class ArtifactPart(BaseModel):
    artifact_id: str
    uri: str  # e.g., "file://./artifacts/{artifact_id}.bin"
    mime_type: str
```

By using `ArtifactPart`, we avoid embedding large blobs of data (e.g., a 10MB generated file) directly into the `history.jsonl`, keeping it lean and performant while maintaining a verifiable link to the data.

## 3. Two-Layer Memory Model

To solve the context window limitation and provide agents with effective long-term memory, we will implement a two-layer memory model.

### Layer 1: The System of Record (`history.jsonl`)

The `history.jsonl` file in the Task Workspace is the **complete, immutable, and auditable history** of the task. It is the ground truth. However, loading the entire history for every LLM call is inefficient and will quickly exceed the context window.

### Layer 2: The Working Memory (Context Compilation)

For each turn, the `Orchestrator` will compile a "working memory" to be sent to the agent. This is a subset of the full history, designed to be relevant to the current task and fit within the LLM's context window.

The key to this layer is the **Context Compilation Strategy**. Initially, this can be a simple "recent N steps" approach. However, the design allows for more sophisticated strategies in the future.

### Active Memory: Tools for Introspection

Crucially, we will empower agents to manage their own memory by providing them with **memory-access tools**. Instead of passively receiving a pre-compiled context, agents can actively query their own history.

- **`memory_search(query: str)`**: A tool that searches the entire `history.jsonl` (potentially using a `Mem0`-like search index built on top of the raw log) for relevant past steps.
- **`artifact_read(uri: str)`**: A tool that reads the content of an artifact referenced in the history.

This approach has several advantages:

- It more closely mirrors how humans work, actively recalling information when needed.
- It keeps the prompt lean by default, only including information the agent explicitly requests.
- It scales infinitely, as the agent is not limited by what can be fit into a single context window.

This hybrid approach gives us the auditability of a file-based system of record and the performance and scalability of an active, indexed memory system.
