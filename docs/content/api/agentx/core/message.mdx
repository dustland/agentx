# message

```python
from agentx.core.message import ...
```

## Classes

### Artifact

```python
class Artifact(BaseModel)
```

Artifact reference with versioning and metadata.


### TextPart

```python
class TextPart(BaseModel)
```

Text content part with language and confidence support.


### ToolCallPart

```python
class ToolCallPart(BaseModel)
```

Tool call request part - conversation representation.


### ToolResultPart

```python
class ToolResultPart(BaseModel)
```

Tool execution result part.


### ArtifactPart

```python
class ArtifactPart(BaseModel)
```

Artifact reference part.


### ImagePart

```python
class ImagePart(BaseModel)
```

Image content part with metadata.


### AudioPart

```python
class AudioPart(BaseModel)
```

Audio content part with metadata.


### MemoryReference

```python
class MemoryReference(BaseModel)
```

Memory reference with relevance scoring.


### MemoryPart

```python
class MemoryPart(BaseModel)
```

Memory operation part.


### GuardrailCheck

```python
class GuardrailCheck(BaseModel)
```

Individual guardrail check result.


### GuardrailPart

```python
class GuardrailPart(BaseModel)
```

Guardrail check results part.


### TaskStep

```python
class TaskStep(BaseModel)
```

A single step in a task's conversation history.


### Message

```python
class Message(BaseModel)
```

Standard chat message format compatible with LLM APIs and Vercel AI SDK.

This follows the industry standard format with role/content/parts structure.

#### Methods

##### user_message

```python
def user_message(cls, content, parts)
```

Create a user message.


##### assistant_message

```python
def assistant_message(cls, content, parts)
```

Create an assistant message.


##### system_message

```python
def system_message(cls, content, parts)
```

Create a system message.



### UserMessage

```python
class UserMessage(Message)
```

User message - alias for Message with role='user'.


### MessageQueue

```python
class MessageQueue(BaseModel)
```

Queue for managing message flow in tasks.

#### Methods

##### add

```python
def add(self, message)
```

Add a message to the queue.


##### get_all

```python
def get_all(self)
```

Get all messages in the queue.


##### clear

```python
def clear(self)
```

Clear all messages from the queue.



### TaskHistory

```python
class TaskHistory(BaseModel)
```

Task execution history with messages and metadata.

#### Methods

##### add_message

```python
def add_message(self, message)
```

Add a message to the history.


##### add_step

```python
def add_step(self, step)
```

Add a task step to the history.



### StreamChunk

```python
class StreamChunk(BaseModel)
```

Token-by-token message streaming from LLM.

This is Channel 1 of the dual-channel system - provides low-latency UI updates for "typing" effect. This is message streaming, not events.


### StreamError

```python
class StreamError(BaseModel)
```

Error in message streaming.


### StreamComplete

```python
class StreamComplete(BaseModel)
```

Message streaming completion marker.


## Functions

### user_message

```python
def user_message(cls, content, parts)
```

Create a user message.


### assistant_message

```python
def assistant_message(cls, content, parts)
```

Create an assistant message.


### system_message

```python
def system_message(cls, content, parts)
```

Create a system message.


### add

```python
def add(self, message)
```

Add a message to the queue.


### get_all

```python
def get_all(self)
```

Get all messages in the queue.


### clear

```python
def clear(self)
```

Clear all messages from the queue.


### add_message

```python
def add_message(self, message)
```

Add a message to the history.


### add_step

```python
def add_step(self, step)
```

Add a task step to the history.

